# gpt4all-experiments
Repo to train / inference different models with GPT4ALL dataset.

This repo is very much a WIP

Currently the following are available:

* hacked together training code for GPT-J and potentially GPT-NeoX
* inference code for GPT-J
* Model LoRA weights available here: [M-Chimiste/GPT-J-6B-GPT4ALL-SAMP-LORA-v1](https://huggingface.co/M-Chimiste/GPT-J-6B-GPT4ALL-SAMP-LORA-v1)
